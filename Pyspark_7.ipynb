{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.master\",\"local[4]\")\n",
    "conf.set(\"spark.driver.memory\",\"16g\")\n",
    "conf.set(\"spark.app.name\",\"sparkapp\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlc  =SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f39caea8e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|score|\n",
      "+---+----+-----+\n",
      "| 20| abc| null|\n",
      "| 30| xyz|  4.5|\n",
      "+---+----+-----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = sqlc.read.format(\"json\").option(\"multiLine\",\"True\").load(\"/home/ubuntu/Notebooks/test.json\")\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd1 = sc.textFile(\"/home/ubuntu/Datasets/ad-events_2014-01-20_00_domU-12-31-39-01-A1-34\")\n",
    "# print(rd1.take(1))\n",
    "# print(rd1.count())\n",
    "rd2 = rd1.map(lambda x:x.split(\" 2014, \"))\n",
    "# print(rd2.take(1))\n",
    "# print(rd2.map(lambda x:len(x)).take(50))\n",
    "# print(rd2.filter(lambda x:len(x)==2).count())\n",
    "rd3 = rd2.map(lambda x:x[1])\n",
    "# print(rd3.take(1))\n",
    "rd4 = rd3.map(lambda x:x.replace(\"}, {\",\",\"))\n",
    "#{   },{   }--->\n",
    "# print(rd4.take(4))\n",
    "\n",
    "r1 = rd1.map(lambda x:x[:30]).map(lambda x:((str)(x),0))\n",
    "r1.take(10)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df1 = sqlc.read.json(rd4)\n",
    "# df1.show(5)\n",
    "# len(df1.columns)\n",
    "r1.take(10)\n",
    "df2 = r1.toDF([\"timezone\",\"misc\"]).drop(\"misc\")\n",
    "# df2.show(5)\n",
    "df3 = df1.withColumn(\"extra1\",monotonically_increasing_id())\n",
    "df4 = df2.withColumn(\"extra2\",monotonically_increasing_id())\n",
    "resdf = df3.join(df4,col(\"extra1\")==col(\"extra2\"),\"inner\").drop(\"extra1\",\"extra2\")\n",
    "# resdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   city| id|\n",
      "+-------+---+\n",
      "|    hyd|  1|\n",
      "|chennai|  2|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   city| id|\n",
      "+-------+---+\n",
      "|    hyd|  1|\n",
      "|chennai|  2|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "r1 = Row(id=1,city=\"hyd\")\n",
    "r2 = Row(id=2,city=\"chennai\")\n",
    "d1 = sqlc.createDataFrame([r1,r2])\n",
    "d1.show()\n",
    "\n",
    "sqlc.read.json(d1.toJSON()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   marks     name\n",
      "0     20    karna\n",
      "1     30     hari\n",
      "2     40  shankar\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "pdf = pandas.DataFrame({\"name\":[\"karna\",\"hari\",\"shankar\"],\"marks\":[20,30,40]})\n",
    "                      \n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|marks|   name|\n",
      "+-----+-------+\n",
      "|   20|  karna|\n",
      "|   30|   hari|\n",
      "|   40|shankar|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sqlc.createDataFrame(pdf)\n",
    "sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "pdf2 = sdf.toPandas()\n",
    "pdf2\n",
    "print(type(pdf2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
